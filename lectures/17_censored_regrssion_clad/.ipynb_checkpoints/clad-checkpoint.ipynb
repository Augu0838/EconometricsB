{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Corner solution outcomes and censored regression:  \n",
    "## Specification Issues and estimation under weaker assumptions\n",
    "\n",
    "### Econometrics B (Ã˜kB)\n",
    "\n",
    "(Wooldridge Ch. 18)\n",
    "\n",
    "Bertel Schjerning\n",
    "\n",
    "Department of Economics, University of Copenhagen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "**Specification issues and estimation under weaker assumptions**\n",
    "- Recap: Estimation of the Tobit Model \n",
    "- Neglected Heterogeneity \n",
    "- Heteroscedasticity and non-normality\n",
    "- Estimation under weaker assumptions\n",
    "- Computational Issues\n",
    "- Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tobit - ML Estimation\n",
    "**Quantities of interest**\n",
    "- **Data censoring:** Latent variable model, $E\\left( y^{\\ast }|x\\right) $or $\\beta $\n",
    "- **Corner Solution:** $P\\left( y=0|x\\right) ,$ $P\\left( y>0|x\\right),E\\left( y|x\\right) ,$ $E\\left( y|x|y>0\\right) $\n",
    " \n",
    "**Tobit model**\n",
    "$$\n",
    "y=\\max \\left( 0,x\\beta +u\\right) \\text{, }u|x\\sim N\\left( 0,\\sigma^{2}\\right) \n",
    "$$\n",
    "\n",
    "Conditional log likelihood contribution\n",
    "$$\\mathcal{L}_i(\\theta) =\n",
    "\\mathbb{1}(y_i=0)\\ln\\left(1 - \\Phi\\left(\\frac{x_i\\beta}{\\sigma}\\right)\\right) +\n",
    "\\mathbb{1}(y_i>0)\\ln\\left(\\dfrac{1}{\\sigma}\n",
    "  \\phi\\left(\\dfrac{y_i-x_i\\beta}{\\sigma}\\right)\\right)$$\n",
    "\n",
    "Inference: Carried out in standard MLE framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neglected Heterogeneity\n",
    "**Suppose we instead are interested in the model**\n",
    "\\begin{eqnarray*}\n",
    "y &=&\\max \\left( 0,x\\beta +\\gamma c+u\\right) \\\\\n",
    "u|x,c &\\sim &N\\left( 0,\\sigma ^{2}\\right) \\\\\n",
    "f\\left( y|x,c\\right) &=&\\left[ 1-\\Phi \\left( \\frac{x\\beta +\\gamma c\n",
    "}{\\sigma }\\right) \\right] ^{I\\left( y=0\\right) }\\left[ \\frac{1}{\\sigma }\\phi\n",
    "\\left( \\frac{y-x\\beta -\\gamma c}{\\sigma }\\right) \\right] ^{I\\left(y>0\\right) }\n",
    "\\end{eqnarray*}\n",
    "\n",
    "**Problem:** Cannot do e.g. CML unless $c$ can be removed\n",
    "\n",
    "\n",
    "**Solution:** Assume distribution for $c$ given $x:h(c|x)$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "f\\left( y|x\\right) &=&E_{c|x}\\left[ f\\left( y|x,c\\right) \\right] \\quad \\text{[integrate out }c\\text{]}\\\\\n",
    "&=&\\left \\{ \n",
    "\\begin{array}{c}\n",
    "\\int \\left[ f\\left( y|x,c\\right) \\right] h\\left( c|x\\right) dc \\quad\\quad \\text{if }\n",
    "c\\text{ is continuous} \\\\ \n",
    "\\sum \\left[ f\\left( y|x,c\\right) \\right] h\\left( c|x\\right) \\quad \\text{ if }\n",
    "c\\text{ is discrete}\n",
    "\\end{array}\n",
    "\\right. \n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 1: Heterogeneity - discrete case\n",
    "Two latent types in population\n",
    "\\begin{eqnarray*}\n",
    "h\\left( c|x\\right) &=&\\left \\{ \n",
    "\\begin{array}{c}\n",
    "\\pi \\quad\\quad\\quad \\text{ if} \\quad c=a_{h}\\text{ (High type)} \\\\ \n",
    "\\left( 1-\\pi \\right) \\quad \\text{if} \\quad  c=a_{l}\\text{ (Low type)}\n",
    "\\end{array}\n",
    "\\right. \n",
    "\\\\\n",
    "f\\left( y|x\\right) &=&\\pi f\\left( y|x,a_{h}\\right) \n",
    "+\\left( 1-\\pi \\right) f\\left( y|x,a_{L}\\right) \n",
    "\\end{eqnarray*}\n",
    "\n",
    "$\\pi ,a_{H},a_{L}$ are parameters to estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 2: Heterogeneity - cont. case\n",
    "**Assume continuous distribution of $c|x$**\n",
    "\n",
    "Assume\n",
    "$$\n",
    "c|x_{i}\\sim N\\left( 0,\\tau ^{2}\\right) \n",
    "$$\n",
    "This implies that $c$ and $x_{i}$ are independent. \n",
    "\n",
    "Therefore\n",
    "\\begin{eqnarray*}\n",
    "y &=&\\max \\left( 0,x\\beta +\\gamma c+u\\right) \\\\\n",
    "&=&\\max \\left( 0,x\\beta +v\\right) \n",
    "\\end{eqnarray*} \n",
    "where $v=\\gamma c+u\\sim N\\left( 0,\\gamma ^{2}\\tau ^{2}+\\sigma ^{2}\\right)$\n",
    "\n",
    "This is simply the standard Tobit model and we can proceed with standard Tobit analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Heteroscedastisity and Non-normality\n",
    "**Severe consequences for Tobit**\n",
    "$\\hat{\\beta}$ is inconsistent for $\\beta$\n",
    "\n",
    "- **Why?**: Because derived density of $y|x$ hinges crucially on \n",
    "$y^{\\ast}|x\\sim N\\left(0,\\sigma_{u}^{2}\\right)$\n",
    "\n",
    "- **Censoring is costly:**: In absence of censoring, we could consistently estimate $\\beta $ by assuming \n",
    "$E(u|x)=0$\n",
    "- **Het. and Non-N changes objects of interest**\n",
    "\t$($in particular $E\\left( y|x\\right) \\ $and $E\\left( y|x,y>0\\right)$)\n",
    "\n",
    "- **Important to test for these misspecification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Initial setup for python illustrations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np \n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# collection of functions used for this lecture on censored regression corner solution outcome models \n",
    "from creg import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simulation exercise \n",
    "### Heteroscedastisity and Non-normality\n",
    "**Consider the model**\n",
    "\\begin{eqnarray*}\n",
    "y &=&\\max \\left( 0,x\\beta +\\gamma c+\\sigma u\\right) \\\\\n",
    "u|x,c &\\sim &F\\left( 0,\\exp(\\delta x_k)\\right) \\\\\n",
    "c &\\sim &N\\left( 0,1\\right) \\\\\n",
    "\\end{eqnarray*}\n",
    "where \n",
    "- F can be a normal distribution, a log normal, a mixture of two normal (symmetric or asymmetric)\n",
    "- we normalize $var(u)=1$ \n",
    "- u is homosecedastic if $\\delta=0$\n",
    "- we have no neglected heterogeneity if $\\gamma=0$\n",
    "\n",
    "Implemented in \n",
    "<code>df, xvars = sim_data()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N=5000\n",
    "# error_type can be 'normal', 'uniform', 'log-normal', 'mix_sym', or 'mix_asym'\n",
    "df, xvars = sim_data(N, beta=[0.5,1], sigma=1, gamma=0, delta=0, error_type='normal') # change parameters \n",
    "fig, (ax) = plt.subplots(1, 2, figsize=(15,7))\n",
    "ax[0].hist(df['y'], bins=100); ax[0].set_title('observed outcome, y')\n",
    "ax[1].hist(df['u'], bins=100); ax[1].set_title('error term, u')\n",
    "res=tobit(df['y'],df[xvars], deriv=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing for Homoscedasticity and Normality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Testing for Homoscedasticity and Normality\n",
    "**Test for homoscedasticity:**\n",
    "- LM test against more general alternatives\n",
    "- e.g. var$\\left( u|x\\right) =\\sigma ^{2}\\exp \\left( z\\delta \\right) $\n",
    "where $z$ is a q-dimensional subvector of $x$ \n",
    "- see Wooldridge for details \n",
    "\n",
    "**Test for normality:**\n",
    "- Conditional moment test by Newey, Tauchen and White \n",
    "- see chapter 13 on conditional moment test for MLE \n",
    "- or next the next slides \n",
    "- Hausman type specification test against more general model \n",
    "(e.g. the Powell estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional moment test for Normality (1/4)}\n",
    "\n",
    "**Newey-Tauchen-White\\ test statistic**\n",
    "$$\n",
    "\\tau =\\iota ^{\\prime}\\hat{M}\\hat{Q}^{-1}\\hat{M}^{\\prime }\\iota \\overset{d}{\\longrightarrow }\\chi ^{2}\\left( r\\right) \n",
    "$$\n",
    "\n",
    "where\n",
    "- $\\iota $ is a $N\\times 1$ vector of ones \n",
    "- $\\hat{M}$ is a $N\\times r$ matrix of sample realizations of the $r$ moment restrictions\n",
    "- $\\hat{Q}$ is a weighting matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional moment test for Normality\n",
    "**Moments**\n",
    "- Let $u_{i}$ be the disturbances from linear model, then $u_{i}=y_{i}^{\\ast}-x_{i}\\beta $   \n",
    "- If $u_{i}$ are normally distributed, then \n",
    "\\begin{eqnarray*}\n",
    "E\\left[ u_{i}^{3}|x_{i}\\right] &=&0  \\\\\n",
    "E\\left[ u_{i}^{4}-3\\sigma ^{4}|x_{i}\\right] &=&0\n",
    "\\end{eqnarray*}\n",
    "\n",
    "in which case $\\hat{M}_{i}$ for the third and the fourth would be\n",
    "$$\n",
    "\\hat{M}_{i}=\\left( \n",
    "\\begin{array}{c}\n",
    "\\hat{u}_{i}^{3} \\\\ \n",
    "\\hat{u}_{i}^{4}-3\\hat{\\sigma}^{4}\n",
    "\\end{array}\n",
    "\\right) ^{\\prime }\n",
    "$$\n",
    "\n",
    "**Problem:** We do not observe $u_{i}$ for $y^{\\ast }\\leq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional moment test for Normality (3/4)\n",
    "**Moments for Tobit**\n",
    "$$\n",
    "\\hat{M}_{i}=\\left( \n",
    "\\begin{array}{c}\n",
    "I_{i}\\hat{u}_{i}^{3}-(1-I_{i})\\left( z_{i}^{2}+2\\right) \\hat{\\sigma}\n",
    "^{3}\\lambda _{i} \\\\ \n",
    "I_{i}\\left( \\hat{u}_{i}^{4}-3\\hat{\\sigma}^{4}\\right) +(1-I_{i})\\left(\n",
    "z_{i}^{2}+3\\right) \\hat{\\sigma}^{4}\\lambda _{i}z_{i}\n",
    "\\end{array}\n",
    "\\right) ^{\\prime }\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "I_{i} &=&1\\left( y>0\\right) \\\\\n",
    "z_{i} &=&x_{i}\\hat{\\beta}/\\hat{\\sigma}\\\\\n",
    "\\lambda _{i} &=&\\phi \\left( x_{i}\\beta /\\sigma \\right) /\\left( 1-\\Phi \\left(x_{i}\\beta /\\sigma \\right) \\right)\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional moment test for Normality (4/4)\n",
    "\n",
    "**Scaling matrix**\n",
    "$$\n",
    "Q=\\left( \\hat{M}-\\hat{S}\\hat{V}\\hat{S}^{\\prime }\\hat{M}\\right) ^{\\prime\n",
    "}\\left( \\hat{M}-\\hat{S}\\hat{V}\\hat{S}^{\\prime }\\hat{M}\\right) ^{\\prime }\n",
    "$$\n",
    "where $\\hat{V}$ can be either of the matrices\n",
    "\\begin{eqnarray*}\n",
    "\\hat{V}_{A} &=&\\left( \\hat{A}\\right) ^{-1}=\\left( \\hat{H}\\right) ^{-1}\\\\\n",
    "\\hat{V}_{B} &=&\\left( \\hat{B}\\right) ^{-1}=\\left( \\hat{S}^{\\prime }\\hat{S}\n",
    "\\right) ^{-1}\\\\\n",
    "\\hat{V}_{Robust} &=&\\hat{A}\\left( \\hat{B}\\right) ^{-1}\\hat{A}=\\hat{H}\\left(\\hat{S}^{\\prime }\\hat{S}\\right) ^{-1}\\hat{H}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimation under weaker assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimation under weaker assumptions\n",
    "**We still consider the model**\n",
    "\\begin{eqnarray*}\n",
    "y^{\\ast } &=&x\\beta +u\\\\\n",
    "y &=&\\max \\left( 0,x\\beta \\right) \n",
    "\\end{eqnarray*}\n",
    "\n",
    "Can we estimate $\\beta $ without imposing normality and independence between $u$ and $x$?\n",
    "\n",
    "**YES!** Under the conditional moment assumption  \n",
    "$$\\text{Med}(u|x)=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Powell's CLAD estimator\n",
    "**Property of median**\n",
    "$$\n",
    "\\text{Med}\\left[g\\left(y\\right)\\right] =g\\left(\\text{Med}\\left[ y\\right]\\right) \n",
    "$$\n",
    "if $g()$ is a (monotone) non-linear function \n",
    "\n",
    "\n",
    "**Therefore:** (nonlinear) median regression\n",
    "\n",
    "$$\n",
    "\\text{Med}\\left[ y|x\\right] =\n",
    "\\max \\left( 0,\\text{Med}\\left[ y^{\\ast }|x\\right] \\right) \n",
    "=\\max \\left( 0, x\\beta+\\text{Med}\\left[ u|x\\right] \\right) \n",
    "=\\max \\left( 0,x\\beta \\right) \n",
    "$$\n",
    "\n",
    "**Hence we can estimate $\\beta$ by solving** \n",
    "$$\n",
    "\\hat{\\beta}_{CLAD}=\\arg \\min_{\\beta }\\sum_{i=1}^{N}\\left \\vert y_{i}-\\max\n",
    "\\left( 0,x_{i}\\beta \\right) \\right \\vert \n",
    "$$ \n",
    "\n",
    "This Estimator was suggested by James Powell and often called the **Powell Estimator** .....(or the **CLAD** estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Identification: Can Powell's CLAD estimate the quantities of interest?\n",
    "**Censored regression**\n",
    "\n",
    "If distribution of $u$ is *symmetric* around zero \n",
    "\n",
    "$$ E\\left( y^{\\ast }|x\\right) =\\text{Med}\\left( y^{\\ast }|x\\right) =x\\beta $$\n",
    "\n",
    "**In corner solution models**\n",
    "- $P\\left( y=0|x\\right) $, $E\\left( y|x\\right) $ and $E\\left(y|x,y>0\\right) $ are not identified from $\\text{Med}\\left( u|x\\right) =0$ \n",
    "- We need further distributional assumptions.\n",
    "- For Tobit all of these objects depends on $\\Phi \\left( {}\\right) )$ \n",
    "- But Powell estimator can be used as a good specification test for the Tobit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Powell's CLAD estimator: Asymptoticss\n",
    "\n",
    "**CLAD is an M-estimator**\n",
    "$$\\hat{\\beta}_{clad}=\\arg \\min_{\\beta} Q_N\\left(\\beta \\right)=\\arg \\min_{\\beta}1/N \\sum_{i=1}^N q\\left( w_{i},\\beta \\right) \n",
    "$$ \n",
    "where \n",
    "$$\n",
    "q( w_i,\\beta) =\\left \\vert y_{i}-\\max \\left(0,x_{i}\\beta \\right) \\right \\vert \n",
    "$$\n",
    "- $\\hat{\\beta}_{clad}$ is **$\\sqrt{N}-$Consistent and asymptotic normal if $\\beta$ is identifiable**\n",
    "\n",
    "But Theorem 12.3 does not hold:\n",
    "- $q\\left( w_{i},\\beta \\right)$ is not twice continuous differentiable in $\\beta $ with non-singular Hessian \n",
    "- $q\\left( w_{i},\\beta \\right)$ has a kink point where $y_{i}=\\max \\left( 0,x_{i}\\beta \\right)$ and $x_{i}\\beta=0$\n",
    "- sample objective function is flat when $x_i\\beta <0$ for all $i$\n",
    "- **Thus we cannot establish $\\sqrt{N}$ asymptotic normality from Thm 12.3**- \n",
    "- Powell has an applicable theorem to establish this result. \n",
    "- You can bootstrap to get standard errors (we do that later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CLAD: Computational Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objective function for CLAD\n",
    "$$\n",
    "Q_{CLAD}(\\beta)=1/N\\sum_{i=1}^{N}\\left \\vert y_{i}-\\max\n",
    "\\left( 0,x_{i}\\beta \\right) \\right \\vert \n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function for CLAD - illustration on next slide\n",
    "def Q_clad(theta, y,x):\n",
    "    return np.mean(np.abs(y-np.maximum(x@theta, 0)))\n",
    "\n",
    "# Need lambda functions for plotting or maximizing objective functions\n",
    "Qn_clad = lambda theta: Q_clad(theta, df['y'],df[xvars])\n",
    "Qn_tobit= lambda theta: Q_tobit(theta, df['y'],df[xvars], out='Q') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computational issues\n",
    "$$\n",
    "\\hat{\\beta}_{CLAD}=\\arg \\min_{\\beta }\\frac{1}{N}\\sum_{i=1}^{N}\\left \\vert y_{i}-\\max\n",
    "\\left( 0,x_{i}\\beta \\right) \\right \\vert \n",
    "$$ \n",
    "1. The objective function is non-differentiable in $\\beta $ at $y_{i}=\\max (0,x_{i}\\beta )$ and  $0=x_{i}\\beta$\n",
    "\t- However, for large $N$ increase these discontinuities are smoothed out \n",
    "\t- This problem with kink points due to $|\\cdot|$ is not specific to the censored regression model, but applies generally for LAD estimators.\n",
    "    - trick: Add logit smoothing\n",
    "\n",
    "1. For some $\\beta ,$ all predicted values of $y^{\\ast }$ are censored, i.e. we have $x_{i}\\beta<0$ for all $i$ \n",
    "\t- If this is the case $|y_{i}-\\max (0,x_{i}\\beta )|$ is not affected by $\\beta $.\n",
    "\n",
    "**Tobit does not suffer from this**: Objective for tobit is perfectly smooth and does not get flat even when model predict that all observations are censored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N=5000  # Number of observations\n",
    "beta0=[-0,1]; sigma0=[1]; ## true parameers\n",
    "df, xvars = sim_data(N, beta0, sigma0, error_type='normal') # simulate data from tobit y=max(x*beta+sigma*e,0)\n",
    "\n",
    "theta0=np.ones(len(beta0))*0 # Starting values \n",
    "output = minimize(Qn_clad, theta0, method='BFGS') # depends on derivatives\n",
    "print('CLAD - BFGS       : ', output.x, output.message)\n",
    "output = minimize(Qn_clad, theta0, method='Nelder-Mead') # does not depend on derivatives\n",
    "print('CLAD - Nelder-Mead: ', output.x,  output.message)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "k=1; interval=[beta0[k]-5,beta0[k]+5]\n",
    "plot_Qn(Qn_clad, beta0, k, interval)\n",
    "plot_Qn(Qn_tobit, beta0+sigma0, k, interval)\n",
    "plt.title('Tobit and CLAD objective, N=' + str(N)); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Add logit smoothing to non-differentialble functions\n",
    "- **logsum** (or **smooth max**) has the max function as limit\n",
    "$$\n",
    " \\lim_{\\lambda \\to 0}\\mathbb{Smax}(z_0,z_1, \\lambda)= \\lim_{\\lambda \\to 0} \\lambda \\log(\\exp(z_0/\\lambda)+\\exp(z_1/\\lambda)) = \\max(z_0,z_1)\n",
    "$$\n",
    "\n",
    "- **logit formula** has the indicator function as limit\n",
    "$$\n",
    "\\lim_{\\lambda \\to 0} \\Lambda(z, \\lambda)=\\lim_{\\lambda \\to 0} \\frac{1}{1+\\exp(-z/\\lambda)} = \\mathbb{1}(z>0)\n",
    "$$\n",
    "\n",
    "- **logit smoothed sign** function $\\mathbb{sign}_{\\lambda}(z)=(2 \\Lambda(z, \\lambda)-1)$  has the sign function as limit (i.e. the function that returms 1 or -1 if z is postive or negative respectility \n",
    "$$\n",
    "\\lim_{\\lambda \\to 0} \\mathbb{sign}_{\\lambda}(z)=\\lim_{\\lambda \\to 0} (2 \\Lambda(z, \\lambda)-1) = \\mathbb{sign}(z)\n",
    "$$\n",
    "\n",
    "- **logit smoothed abs** function $\\vert z \\vert_\\lambda=\\mathbb{sign}_{\\lambda}(z) \\cdot z$  has the abs function as limit \n",
    "$$\n",
    "\\lim_{\\lambda \\to 0} \\vert z \\vert_\\lambda= \\lim_{\\lambda \\to 0} \\mathbb{sign}_{\\lambda}(z)z = \\vert z \\vert\n",
    "$$\n",
    "- suggests **smoothed clad objective** \n",
    "$$\n",
    "\\lim_{\\lambda \\to 0} Q_{N}^{CLAD_{\\lambda}}(\\beta, \\lambda)\n",
    "= \\lim_{\\lambda \\to 0} \\frac{1}{N} \\sum_{i=1}^{N}\n",
    "\\vert \n",
    "(y_i-\\mathbb{Smax}(0,x_{i}\\beta, \\lambda)\n",
    "\\vert_\\lambda\n",
    "= \\frac{1}{N} \\sum_{i=1}^{N}\\left \\vert \n",
    "y_{i}-\\max\n",
    "(0,x_{i}\\beta) \n",
    "\\right \\vert \n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def logit(v, bw): \n",
    "    return 1/(1+np.exp(-v/bw))\n",
    "def smoothabs(v, bw): \n",
    "    p=logit(v, bw);\n",
    "    return (2*p-1)*v\n",
    "def smoothmax(v0,v1, bw): \n",
    "    vmax= v0+np.maximum(v1-v0,0)\n",
    "    return vmax + bw*np.log(np.exp((v0-vmax)/bw)+np.exp((v1-vmax)/bw))\n",
    "\n",
    "def Q_clad_smooth(theta, y,x, bw=0.1):\n",
    "    xb=x@theta\n",
    "    return np.mean(smoothabs(y-smoothmax(0, xb, bw), bw))\n",
    "\n",
    "z=-1\n",
    "print(logit(z, .1))\n",
    "print(smoothmax(0,z, .1))\n",
    "print(smoothabs(z, .1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Smoothed clad estimator\n",
    "$$\n",
    "\\hat{\\beta_\\lambda}=\\arg \\min_{\\beta} \\frac{1}{N} \\sum_{i=1}^{N}\n",
    "\\vert \n",
    "(y_i-\\mathbb{Smax}(0,x_{i}\\beta, \\lambda)\n",
    "\\vert_\\lambda\n",
    "$$\n",
    "The objective function \n",
    "$$ \n",
    "Q_{N}^{CLAD_{\\lambda}}(\\beta, \\lambda) = \\frac{1}{N} \\sum_{i=1}^{N}\\vert (y_i-\\mathbb{Smax}(0,x_{i}\\beta, \\lambda)\\vert_\\lambda\n",
    "$$ \n",
    "- is smooth and differentiable in $\\beta $ everywhere for $\\lambda>0$\n",
    "- the smoothed CLAD estimator, $\\hat{\\beta_\\lambda}$ introduces smoothing bias, but is consistent if $\\lambda \\to 0$ as $N \\to \\infty$\n",
    "- the principle of *kernel smoothing* is used in many contexts (e.g. non-parametric regression, tax-systems with kinks, simulators for discrete choice models, etc.)\n",
    "- choosing $\\lambda$: trade-off between smoothing bias and computational issues (and between bias and variance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N=5  # Number of observations\n",
    "beta0=[0,1]; sigma0=[1]; ## true parameters\n",
    "df, xvars = sim_data(N, beta0, sigma0, error_type='normal') # simulate data from tobit y=max(x*beta+sigma*e,0)\n",
    "Qn_clad_smooth = lambda theta: Q_clad_smooth(theta, df['y'],df[xvars], bw=0.1)\n",
    "\n",
    "theta0=np.ones(len(beta0))*0 # Starting values \n",
    "output = minimize(Qn_clad, theta0, method='BFGS'); print('CLAD - BFGS       : ', output.x, output.message)\n",
    "output = minimize(Qn_clad_smooth, theta0, method='BFGS'); print('CLAD - SMOOTH     : ', output.x, output.message)\n",
    "output = minimize(Qn_clad, theta0, method='Nelder-Mead'); print('CLAD - Nelder-Mead: ', output.x,  output.message)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "k=1; interval=[beta0[k]-5,beta0[k]+5]\n",
    "plot_Qn(Qn_clad, beta0, k, interval)\n",
    "plot_Qn(Qn_clad_smooth, beta0, k, interval)\n",
    "plt.title('CLAD and smoothed CLAD objective, N=' + str(N)); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to obtain CLAD estimator without smoothing\n",
    "**When objective function have flat regions or discontinuous derivatives**\n",
    "It can be hard to maximize the likelihood function using gradient based maximization methods for small $N$\n",
    "- BGFS, Newton-Raphson and BHHH often fails for small $N$ $\\to$ use Nelder-Mead\n",
    "- most algorithms would fail, if started at the flat segment\n",
    "\n",
    "**Alternatively**\n",
    "We can use an estimation technique for the Powell estimator based on an iterative linear programming algorithm (ILPA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iterative linear programming algorithm (ILPA) \n",
    "\n",
    "**Steps for ILPA**\n",
    "1. Estimate a median regression for the full sample\n",
    "1. Delete the observations for which the predicted value of the dependent variable is less than zero.\n",
    "1. Iterate on 1) and 2) until convergence\n",
    "\n",
    "That is\n",
    "- Another median regression is estimated on the new sample, and again negative predicted values are dropped.\n",
    "\n",
    "Buchinsky (1991) shows that *if the process converges*, then a local minimum is obtained.\n",
    "\n",
    "- Convergence occurs when there are no negative predicted values in two consecutive iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iterative linear programming algorithm (ILPA).\n",
    "**The median regression in step 1**\n",
    "- When estimating the median regressions we essentially solve the\n",
    "problem $\\min_{\\tilde{\\beta}}\\frac{1}{n}\\sum_{i=1}^{N}|y_{i}-x_{i}\\tilde{\\beta}|$\n",
    "- by reformulating the minimization problem to a linear programming\n",
    "problem and solve this using a simplex based method.\n",
    "- We use the quantile regression routine from statsmodels.QuantReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clad_ilpa(yvar, xvars, maxiter_ilpa=20, quiet=False, b=None):  \n",
    "    x=np.array(xvars)\n",
    "    N, K, xvars=labels(x)\n",
    "    y=np.array(yvar).reshape(N,1)\n",
    "\n",
    "    if np.mean(1*(y>0))<0.5:\n",
    "        print('Error: More than half of observations are censored. Beta is unidentified')\n",
    "        return np.nan*np.ones((K,1))\n",
    "\n",
    "    if b is None: # use lad estimates as starating values\n",
    "        lad=sm.QuantReg(y,x).fit(q=0.5, p_tol=1e-05)\n",
    "        b=np.array(lad.params).reshape(-1,1)\n",
    "\n",
    "    for i in range(maxiter_ilpa):\n",
    "        b0=b;\n",
    "        yhat=x@b\n",
    "        lad=sm.QuantReg(y[yhat>0],x[yhat[:,0]>0,:]).fit(q=0.5,p_tol=1e-05)\n",
    "        b=np.array(lad.params).reshape(-1,1)\n",
    "\n",
    "        if (b==b0).all():  # \"convrgence achieved\";\n",
    "            if quiet==False:\n",
    "                print('CLAD finished in %d iterations using ILPA' %i)\n",
    "                print('Fractions of observations that are censored: ', np.mean(1*(y==0)))\n",
    "                print(lad.summary())    \n",
    "            return np.array(b).reshape(-1,1)\n",
    "    \n",
    "    return np.nan*np.ones((K,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## NOTE DTANDAERD ERRORS BELOW ARE NOT VALID (out put is simply from the last LAD estimation from ILPA )\n",
    "df, xvars = sim_data(N=10000, beta=[.5,2], sigma=1, delta=1, error_type='normal') # simulate data from tobit y=max(x*beta+sigma*e,0)\n",
    "b=clad_ilpa(df['y'], df[xvars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bootstrapped standard errors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bootstrapping\n",
    "There are two ways of obtaining standard errors for an estimate:\n",
    "- Asymptotics, e.g. Theorem 12.3 for M-Estimators\n",
    "- Bootstrrapping\n",
    "\n",
    "**Bootstrapping:** \n",
    "- Method for estimating distribution of estimator or test statistic by re-sampling one's data \n",
    "- Similar to Monte Carlo, but treating the observed sample as if it was the population.\n",
    "- Bootstrapping is an easy way to obtain\n",
    "\t- Standard errors\n",
    "\t- Confidence intervals\n",
    "\t- Critical values and p-values for test statistics\n",
    "- Why and when Bootstrapping works is much difficult and outside the curriculum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why the bootstrap?\n",
    "- Sometimes difficult to derive asymptotic distribution of an estimator or statistic, e.g. 2-step estimators, CLAD or APE's (or other statistics derived from parameter estimates)\n",
    "- Asymptotic results may be very inaccurate in finite samples\n",
    "\t- very difficult to derive small sample properties of estimators.\n",
    "\t- Bootstrap often approximates the distribution of an estimator or test statistic well and often more accurate in finite samples.\n",
    "\n",
    "### Why not bootstrap?\n",
    "- Computational expensive \n",
    " \n",
    " model has to be estimated many times (say more than 500 times)\n",
    "- Bootstrap's numerical performance may be poor, when estimators whose asymptotic covariance matrices are \"nearly singular (for example IVE with weak instruments)\n",
    "- Bootstrap is sometimes biased and should not be used blindly or uncritically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why is the method called the bootstrap?\n",
    "\n",
    "*Baron Munchausen is both a historical and literary character. The historical\n",
    "character lived in Germany in the 18th century. When he was young he joined\n",
    "the Russian army where he became captain. He retired at the age of 30 and\n",
    "lived the rest of his life at his manor. He is known to have told witty and\n",
    "exaggerated stories mainly about his time in the Russian army. In one of\n",
    "these stories, he falls into a swamp and cannot get up. According to the\n",
    "story he pulled himself up by the bootstrap.*\n",
    "\n",
    "Note: \n",
    "There actually seems to be disagreement whether he pulled himself up by the bootstrap or the hair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Road map, Bootstrapping\n",
    "\n",
    "Several types of bootstrap (when data is iid)\n",
    "- **nonparametric bootstrap** (we will do this)\n",
    "- parametric Bootstrap \n",
    "- non-parametric residual bootstrap\n",
    "\n",
    "**Block bootstrap (Panel data)**\n",
    "- As above but here it is the cross-sectional units we sample randomly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The nonparametric bootstrap\n",
    "The algorithm for the nonparametric bootstrap is:\n",
    "\n",
    "1. Estimate model on original sample to obtain the statistic $T_{N}$\n",
    "1. Draw a random samples (with replacement) of size $N$.\n",
    "1. Use the bootstrap sample to obtain an estimate $T_{N,b}^{\\ast }$.\n",
    "1. Repeat 2)-3) many times to obtain a sequence of bootstrap estimates, $T_{N,b}^{\\ast }$, $b=1,..,B$.\n",
    "\n",
    "- Calculate for example the standard deviation of the $B$ values of $\n",
    "T_{N,b}^{\\ast }$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Statistics\n",
    "\n",
    "**The sample variance**\n",
    "\\begin{equation*}\n",
    "\\frac{1}{B-1}\\sum_{b=1}^{B}\\left[ T_{N,b}^{\\ast }-\\bar{T}_{N}^{\\ast }\\right] \n",
    "\\left[ T_{N,b}^{\\ast }-\\bar{T}_{N}^{\\ast }\\right] ^{\\prime }\n",
    "\\end{equation*}\n",
    "where, $\\bar{T}_{N}^{\\ast }$ is the mean of the $B$ bootstrap statistics $\n",
    "T_{N,b}^{\\ast }$, can be used to obtain standard errors for $T_{N}$.\n",
    "\n",
    "- **95 \\% confidence intervals**: \n",
    "\n",
    " can be obtain by finding $0.025$ and $0.975$ percentiles in the bootstrap distribution of $T_{N}^{\\ast }$\n",
    "\n",
    "- **$p$-value of a test statistic**: \n",
    "\n",
    " fraction of times the bootstrap test-statistics exceeding the test statistic from the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Block Bootstrap (for panel data)\n",
    "\n",
    "The bootstrap procedure for panel data:\n",
    "1. Estimate panel data model on original sample to obtain $T_{N}$.\n",
    "1. The bootstrap sample is created by repeatedly drawing (with\n",
    "replacement) an *individual*\n",
    "\t- including all $T$ observations for the drawn individual\n",
    "\t- the draws continue until the sample size $NT$ is reached.\n",
    "1. Use the bootstrap sample to obtain an estimate $T_{N,b}^{\\ast }$.\n",
    "1. Repeat 2)-3) $B$ times to obtain a sequence of bootstrap estimates, $\n",
    "T_{N,b}^{\\ast }$, $b=1,..,B$.\n",
    "1. Calculate for example the standard deviation of the $B$ values of $\n",
    "T_{N,b}^{\\ast }$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pyhon: Bootstrap vs Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(df, estimator, nB=1): # sample from data set with replacement\n",
    "    theta_hat=estimator(df)\n",
    "    N=df.shape[0]\n",
    "    nP=theta_hat.shape[0]\n",
    "    thetas_B=np.empty((nP, nB))\n",
    "    \n",
    "    for iB in range(nB):\n",
    "        ## bootstrap sample (sample from data set with replacement)\n",
    "        dfB=df.sample(N, replace=True)\n",
    "        thetas_B[:,iB]=estimator(dfB, theta0=theta_hat).T\n",
    "        \n",
    "    print('parameter estimate   ', theta_hat.reshape(1,nP).round(5))\n",
    "    print('E(theta_B)           ', np.nanmean(thetas_B, axis=1).round(5))\n",
    "    print('bootstrap SE         ', np.nanstd(thetas_B, axis=1).round(5))\n",
    "    print('faction not converged', np.mean(thetas_B==np.nan))\n",
    "    return thetas_B\n",
    "\n",
    "def montecarlo(dgp, estimator, nMC=1, theta0=None): # sample from data generating process (dgp)\n",
    "    df, xvars = dgp()\n",
    "    theta_hat=estimator(df)\n",
    "    N=df.shape[0]\n",
    "    thetas_MC=np.empty((theta_hat.shape[0], nMC))\n",
    "    \n",
    "    for iMC in range(nMC):\n",
    "        ## Monte carlo sample (sample from DGP)\n",
    "        dfMC, xvars=dgp()\n",
    "        thetas_MC[:,iMC]=estimator(dfMC, theta0=theta_hat).T\n",
    "\n",
    "    print('E(theta_MC)          ', np.nanmean(thetas_MC, axis=1))\n",
    "    print('MCSE                 ', np.nanstd(thetas_MC, axis=1).round(5))\n",
    "    print('faction not converged', np.mean(thetas_MC==np.nan).round(5))\n",
    "    \n",
    "    return thetas_MC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Illustration: Tobit vs CLAD\n",
    "Goal: Find bootstrap and Monte Carlo standard errors for Tobit and CLAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def b_tobit(df, theta0=None):\n",
    "    res=tobit(df['y'], df[xvars], theta0=theta0, quiet=True); \n",
    "    return res.theta_hat.reshape(-1,1)\n",
    "\n",
    "def b_clad(df, theta0=None): return clad_ilpa(df['y'], df[xvars], maxiter_ilpa=20, b=theta0, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dgp =lambda: sim_data(N=5000, beta=[0,1], sigma=[1], delta=[3], error_type='normal') \n",
    "df, xvars=dgp()\n",
    "res=tobit(df['y'], df[xvars])\n",
    "nrep=500 # Number of bootstrap/monte carlo samples \n",
    "print('Tobit (beta and sigma):'),\n",
    "tobit_B=bootstrap(df   , b_tobit, nB=nrep)\n",
    "tobit_MC=montecarlo(dgp, b_tobit, nMC=nrep)\n",
    "print('\\nCLAD (only beta):')\n",
    "clad_B=bootstrap(df   , b_clad, nB=nrep)\n",
    "clad_MC=montecarlo(dgp, b_clad, nMC=nrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot montecarlo and bootstrap distribution of parameters\n",
    "# Does distribution look approximately standard normal as theory would predict?\n",
    "def plot_histogram(thetas, title='parameter'): \n",
    "        plt.hist(thetas, bins=30)\n",
    "        plt.title(title)\n",
    "            \n",
    "plot_histogram(tobit_MC[1,:], 'Tobit, slope parameter ')\n",
    "plot_histogram(tobit_B[1,:], 'Tobit, slope parameter ')\n",
    "plt.show()\n",
    "plot_histogram(clad_MC[1,:], 'CLAD, slope parameter ')\n",
    "plot_histogram(clad_B[1,:], 'CLAD, slope parameter ')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Concluding remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concluding remarks\n",
    "\n",
    "- The tobit model offers an **appropriate specification** under normality and homoscedasticity\n",
    "for **censored variables**.\n",
    "- It relies on **strong distributional assumptions** for **consistency** of MLE.\n",
    "- These assumptions are testable (conditional moment test, LM or LR tests against more general alternatives)  \n",
    "- **Semi-parametric approaches** that relax the distributional assumptions\n",
    "are possible: Powell's CLAD estimator\n",
    "    - CLAD always breaks down when more that half the sample is censored.\n",
    "    - CLAD can at best identify $E(y*|x)$ but not $E(y|x)$\n",
    "    - Objective function for CLAD is non-differentiable - but is smoothed out in the limit as $N \\to \\infty$\n",
    "    - Possible to use smoothed version of CLAD, but this may introduce bias\n",
    "    - Inference CLAD is non-standard (possible to use bootstrap) "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "date": 1602643870.398518,
  "filename": "38_optimization.rst",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "title": "Econometrics B #13"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
