{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sample Selection in regression models\n",
    "\n",
    "\n",
    "### Econometrics B (Ã˜kB)\n",
    "\n",
    "(Wooldridge Ch. 18)\n",
    "\n",
    "Bertel Schjerning\n",
    "\n",
    "Department of Economics, University of Copenhagen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "**Lectures on sample selection**\n",
    "- Sample selection in regression models\n",
    "- Exclusion restrictions\n",
    "- Likelihood models\n",
    "- Nonparametric bounds: Set vs point identification in the sample selection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sample selection due to sample design\n",
    "- Sample selection based on an explanatory variable\n",
    "\t- Example: only persons aged 30-50 years are interviewed.\n",
    "- Truncation:\n",
    "\t- Sample selected based on the values of the dependent variable.\n",
    "\t- Ex.: We want to explain wealth and only poor people is sampled.\n",
    "\n",
    "### Sample selection due behavior:\n",
    "- *Non-response on survey questions*\n",
    "- *Attrition*: People drop out of the sample over time\n",
    "- *Incidental truncation*: Dependent variable unobserved because of the outcome of another variable (Classical example:  we only observe wages for who work)\n",
    "\n",
    "#### When is sample selection a problem?\n",
    "Problems arises when the selection a non-random drawn from the population of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sample selection framework for regression models\n",
    "Consider the linear regression model\n",
    "\n",
    "$$\ty_{1}=x_{1}\\beta _{1}+u_{1},\\quad \\quad E( u_{1}|x_{1}) =0 $$\n",
    "\n",
    "**The sample selection problem**\n",
    "- $y_{1}$ or $x_{1}$ or both are unobserved when some selection indicator $s=0$\n",
    "- If we run a regression on the selected sample, we effectively condition on $s=1$\n",
    "\n",
    "**Implication**\n",
    "- We will have to work with the regression function $E(y_{1}|x_{1},s=1) $\n",
    "- We need to condition on $s=1$, but object of interest is $E( y_{1}|x_{1}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sample selection framework for regression models\n",
    "\n",
    "\n",
    "<img src=\"img/sampleselection.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sample selection: 5 cases\n",
    "**Equation of interest:**\n",
    "$$\n",
    "\ty_{1}=x_{1}\\beta _{1}+u_{1},\\quad \\quad E( u_{1}|x_{1}) =0\n",
    "$$\n",
    "\n",
    "We consider **5 types of sample selection:**\n",
    "1. $s$ is a function of $x_{1}$ only\n",
    "1. $s$ is independent of $x_{1}$, and $u_{1}$\n",
    "1. $s=1( a_{1}<y_{1}<a_{2}) $ (truncation)\n",
    "1. $s=1( x\\delta_{2}+v_{2}>0) $ (discrete response selection with dependence between $u_{1}$ and $v_{2}$)\n",
    "\n",
    "1. $y_{2}=\\max (0,x\\delta_{2}+v_{2}) $ and $s=1(y_{2}>0)$ (Tobit selection with dependence between $u_{1}$ and $v_{2}$ - implies more structure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 1: Selection on a regressor\n",
    "**Assume selection is a deterministic function of $x_{1}$ only**\n",
    "\n",
    "$$ s=h\\left( x_{1}\\right) $$\n",
    "\n",
    "Regression model on the selected sample\n",
    "\n",
    "$$ E(y_{1}|x_{1},s=1) =x_{1}\\beta _{1}+E(u_{1}|x_{1},s=1) $$\n",
    "\n",
    "- Since $s$ is a deterministic function of $x_{1}$, $s$ does not give us more information than $x_{1}$\n",
    "\n",
    "$$\tE(u_{1}|x_{1},s=1) =E(u_{1}|x_{1}) $$\n",
    "\n",
    "- Assuming exogenous explanatory variables $E\\left( u_{1}|x_{1}\\right)=0$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\tE(y_{1}|x_{1},s=1)  &=&x_{1}\\beta _{1}+E(u_{1}|x_{1}) \\\\\n",
    "\t&=&x_{1}\\beta _{1}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 1: We can ignore selection when selection is a deterministic function of $x_1$\n",
    "- Selection rule: deterministic function of $x_1$\n",
    "\n",
    "$$ s=h(x_{1}) $$\n",
    "\n",
    "- We can consistently estimate $\\beta $ (as well as $E\\left(y_{1}|x_{1}\\right) $) using OLS on the selected sample, since \n",
    "\n",
    "$$ E\\left( y_{1}|x_{1},s=1\\right) =x_{1}\\beta _{1}$$\n",
    "\n",
    "- **Selection on a regressor is not a problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 2: Selection independent of x and u\n",
    "\n",
    "Assume selection is independent of $x_{1}$ and $u_{1}$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "E( y_{1}|x_{1},s=1)  &=&x_{1}\\beta _{1}+E(u_{1}|x_{1},s=1) \\\\\n",
    "\t&=&x_{1}\\beta _{1}+E( u_{1}|x_{1}) \\\\\n",
    "\t&=&x_{1}\\beta _{1}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "$\\to$ we can consistently estimate $\\beta $ (as well as $E(y_{1}|x_{1}) $) using OLS on the selected sample, since \n",
    "\n",
    "$$\tE(y_{1}|x_{1},s=1) =x_{1}\\beta _{1} $$\n",
    "\n",
    "**$\\to$ Random sample selection is not a problem**\t\n",
    "-  Example: Sometimes, we (are forced to) make a *random* subsample of our dataset, and the result shows that estimating on our subsample will give us consistent estimates.\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 3: Truncated regression: Selection on the response variable (selection on $y_{1}$)\n",
    "\n",
    "Suppose that the selection rule is\n",
    "$$ s=\\mathbb{1}[a_{1} < y_{1} < a_{2}] $$\n",
    "\n",
    "- $\\left( y_{1},x_{1}\\right) $ only observed if $s=1$.\n",
    "- $a_{1}$ and $a_{2}$ are  {known} constants and obviously $a_{2}>a_{1}$.\n",
    "\n",
    "**We do not need to have truncation from both sides**\n",
    "- Truncation only from below corresponds to $a_{2}=\\infty $\n",
    "- Truncation only from above corresponds to $a_{1}=-\\infty $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 3: Selection can't be ignored\n",
    "**Object of interest:** As usual we are interested in estimating \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "E\\left( y_{1}|x_{1}\\right) =x_{1}\\beta _{1} \\\\\n",
    "s=1[ a_{1} < y_{1} < a_{2}] \n",
    "\\end{eqnarray*}\n",
    "\n",
    "- Conditional moment restrictions are not sufficient for identification\n",
    "- Need to specify full conditional distribution of $y_{1}$ given $x_{1}$\n",
    "- Need to estimate using CMLE\n",
    "\n",
    "**Assumption:** Conditional cdf $y_{1}|x_{1}$ is $F(c|x_{1})=P(y_{1}\\leq c|x_{1}) $, where $c$ is the usual dummy argument.\n",
    "\n",
    "- Can't work with distribution of $y_{1}|x_{1}$ directly\n",
    "- Need to derive distribution condition on selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Digression: conditional density\n",
    "Recall, the formula for conditional probability\n",
    "$$\n",
    "\tP\\left(A|B\\right) =\\frac{P\\left( A\\cap B\\right) }{P\\left( B\\right) }\n",
    "$$ \n",
    "we can now write\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P\\left( y_{1}\\leq c|x_{1},s=1\\right)  &=&\\frac{P\\left( y_{1}\\leq c,s=1|x_{1}\\right) }{P\\left( s=1|x_{1}\\right) }\\\\\n",
    "&=&\\frac{P\\left( a_{1} < y_{1} < c|x_{1}\\right) }{P\\left(a_{1} < y_{1} < a_{2}|x_{1}\\right) } \\\\\n",
    "&=&\\frac{F\\left( c|x_{1}\\right) -F\\left( a_{1}|x_{1}\\right) }{F\\left(a_{2}|x_{1}\\right) -F\\left( a_{1}|x_{1}\\right) }\n",
    "\\end{eqnarray*}\n",
    "\n",
    "To obtain density simply differentiate with respect to $c$\n",
    "$$\n",
    "\tf\\left( c|x_{1},s=1\\right) =\\frac{f\\left( c|x_{1}\\right) }{F\\left(a_{2}|x_{1}\\right) -F\\left( a_{1}|x_{1}\\right) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood function for truncated regression (selection $y_1$)\n",
    "\n",
    "Plug $y_{1i}$ into this and take the logs, then we arrive at the log likelihood function\n",
    "\\begin{eqnarray*}\n",
    "\\ln L(\\beta ,\\sigma)  \t&=&\\frac{1}{N}\\sum_{i=1}^{N}s_{i}\\ln f(y_{1i}|x_{1i},s_{i}=1) \\\\\n",
    "\t\t\t\t&=&\\frac{1}{N}\\sum_{i=1}^{N}s_{i}\\ln \\left( \\frac{f\\left( y_{1i}|x_{1i}\\right) }{F\\left( a_{2}|x_{1i}\\right) -F\\left( a_{1}|x_{1i}\\right) }\\right) \n",
    "\\end{eqnarray*}\n",
    "\n",
    "If $y_{1}|x_{1}\\sim N\\left( x_{1}\\beta _{1},\\sigma ^{2}\\right) $ and $a$ is independent of $x_{1}$ we have the **truncated** Tobit model.\n",
    "$$\n",
    "\\ln L\\left( \\beta ,\\sigma \\right) =\\frac{1}{N}\\sum_{i=1}^{N}s_{i}\\ln \\frac{\\frac{1}{\\sigma }\\phi \\left( x_{1i}\\beta \\right) }{\\Phi \\left( a_{2}\\right) -\\Phi\\left( a_{1}\\right) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 3: Truncated vs. censored Tobit \n",
    "\n",
    "**Censored Tobit:** we observe covariates $x_{1}$ for all people\n",
    "\n",
    "**Truncated Tobit:** we do not.\n",
    "\n",
    "- If we have data to use the censored Tobit, we would use this, since then we will use all our information\n",
    "\n",
    "- As in censored regression, heteroscedasticity and non-normality has severe consequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 4: Incidental truncation (Probit selection)\n",
    "**Structural equation**\n",
    "$$\n",
    "y_{1}=x_{1}\\beta _{1}+u_{1},\\ E\\left( u_{1}|x_{1}\\right) =0\n",
    "$$\n",
    "\n",
    "**Reduced form probit selection equation**\n",
    "\n",
    "$$\n",
    "s=1\\left( x\\delta_{2}+v_{2}>0\\right) \n",
    "\\quad \\quad v_{2}\\sim N\\left( 0,1\\right)\n",
    "\\quad \\quad x=\\left( x_{1},x_{2}\\right)\n",
    "$$\n",
    "- $\\left( u_{1},v_{2}\\right) $ is independent of $x$\n",
    "\n",
    "- We only observe $y_{1}$ when $s=1,$ but $x$ is always observed\n",
    "\n",
    "- Selection equation is a probit equation $P\\left(s=1|x\\right) =\\Phi\\left(x\\delta_{2}\\right)$ \n",
    "\n",
    "\n",
    "\n",
    "- **Assume further that $u_{1},v_{2}$ are mutually mean dependent**,  \n",
    "$E\\left( u_{1}|v_{2}\\right) =\\gamma _{1}v_{2}$\n",
    " \n",
    " $\\to$ This form for dependence rules out many joint distributions\n",
    "\n",
    " $\\to$ but not the bivariate normal distribution.\n",
    "\n",
    " $\\to$ less restrictive than assuming that $u_{1}$ and $v_{2}$ are bivariate normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Case 4: Regression on the selected sample\n",
    "\n",
    "To derive $E\\left( y_{1}|x,s=1\\right) $ we first compute\n",
    "\n",
    "$$\n",
    "E\\left( y_{1}|x,s=1,v_{2}\\right) \n",
    "=x_{1}\\beta _{1}+E\\left( u_{1}|x_{1},v_{2}\\right) \n",
    "$$\n",
    "\n",
    "Because $u_{1}$ and $v_{2}$ are independent of $x$ we can write\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "E\\left( y_{1}|x,s=1,v_{2}\\right)  &=&x_{1}\\beta _{1}+E\\left(\n",
    "u_{1}|v_{2}\\right) \n",
    "\\\\\n",
    "&=&x_{1}\\beta _{1}+\\gamma _{1}v_{2}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where we have used the assumption that $E\\left( u_{1}|v_{2}\\right) =\\gamma\n",
    "_{1}v_{2}$\n",
    "\n",
    "**By LIE we obtain the conditional regression function**\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "E\\left( y_{1}|x,s=1\\right)  &=&x_{1}\\beta +E\\left( \\gamma\n",
    "_{1}v_{2}|x,s=1\\right) \\\\\n",
    "&=&x_{1}\\beta +\\gamma _{1}E\\left( v_{2}|x,v_{2}>-x\\delta_{2}\\right) \n",
    "\\end{eqnarray*}\n",
    "\n",
    "Regression depends on $E\\left( v_{2}|x,v_{2}>-x\\delta_{2}\\right)$ which takes the form of a mean of a truncated Normal variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 4: When is selection due to incidental truncation an issue?\n",
    "- If $u_{1}$ and $v_{2}$ are uncorrelated\n",
    "- If $\\gamma_{1}=0$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "E\\left( y_{1}|x,s=1\\right)  &=&x_{1}\\beta _{1}+\\gamma _{1}E\\left(\n",
    "v_{2}|x,v_{2}>-x\\delta_{2}\\right) \\beta _{1}\\\\\n",
    "&=&x_{1}\\beta _{1}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- There is no selection problem and we can just use OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 4: Incidental truncation (Probit selection)\n",
    "- If $u_{1}$ and $v_{2}$ are correlated\n",
    "- If $\\gamma_{1}\\neq 0$\n",
    "\n",
    "$$\n",
    "E\\left(y_{1}|x,s=1\\right) \n",
    "=x_{1}\\beta_{1}+\\gamma_{1}E\\left(v_{2}|x,v_{2}>-x\\delta_{2}\\right) \n",
    "$$\n",
    "\n",
    "- OLS on the selected sample is inconsistent due to the selection bias $\n",
    "E\\left( v_{2}|x,v_{2}>-x\\delta_{2}\\right) >0$\n",
    "\n",
    "- $E\\left( v_{2}|x,v_{2}>-x\\delta_{2}\\right)$: omitted variable correlated with $x$\n",
    "\n",
    "- Notice that $v_{2}\\sim N\\left( 0,1\\right)$ \n",
    "\n",
    "- $E\\left( v_{2}|x,v_{2}>-x\\delta_{2}\\right) $ is just the mean of a\n",
    "truncated normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Digression: Expected value of a truncated normal\n",
    "\n",
    "First note that when $v_{2}\\sim N\\left( 0,1\\right)$ \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\phi \\left( v_{2}\\right)  &=&\\frac{1}{\\sqrt{2\\pi }}\\exp \\left(-\\frac{v_{2}^{2}}{2}\\right) \\\\\n",
    "\\phi ^{\\prime }\\left( v_{2}\\right) \n",
    "&=&-v_{2}\\frac{1}{\\sqrt{2\\pi }}\\exp \\left( -\\frac{v_{2}^{2}}{2}\\right) \n",
    "=-v_{2}\\phi \\left( v_{2}\\right) \n",
    "\\end{eqnarray*}\n",
    "\n",
    "**Expected value of a truncated standard normal**\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "E\\left( v_{2}|v_{2}>-x\\delta_{2}\\right) \n",
    "&=&\\frac{\\int_{-x\\delta_{2}}^{\\infty }v_{2}\\phi \\left( v_{2}\\right) dv_{2}}{\n",
    "1-\\Phi \\left( -x\\delta_{2}\\right) }\n",
    "=\\frac{-\\int_{-x\\delta_{2}}^{\\infty }\\phi ^{\\prime }\\left( v_{2}\\right)\n",
    "dv_{2}}{1-\\Phi \\left( -x\\delta_{2}\\right) }\\\\\n",
    "&=&\\frac{\\left[ -\\phi \\left( \\infty \\right) --\\phi \\left( -x\\delta\n",
    "_{2}\\right) \\right] }{1-\\Phi \\left( -x\\delta_{2}\\right) }\n",
    "=\\frac{\\phi \\left( x\\delta_{2}\\right) }{\\Phi \\left( x\\delta_{2}\\right) }\n",
    "\\end{eqnarray*}\n",
    "\n",
    " - We have used that the normal distribution is symmetric \n",
    "such that $\\phi \\left( x\\delta_{2}\\right) =\\phi \\left( -x\\delta_{2}\\right)$ and $\\Phi \\left( x\\delta_{2}\\right) =1-\\Phi \\left( -x\\delta_{2}\\right)$ \n",
    "\n",
    "- Need to normalize density with probability for truncation so that normalized density integrates to $1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 4: Collecting terms\n",
    "\\begin{eqnarray}\n",
    "E\\left( y_{1}|x,s=1\\right)  &=&x_{1}\\beta +\\gamma_{1}\\frac{\\phi \\left(\n",
    "x\\delta_{2}\\right) }{\\Phi \\left( x\\delta_{2}\\right) }  \\notag \\\\\n",
    "&=&x_{1}\\beta +\\gamma_{1}\\lambda \\left( x\\delta_{2}\\right) \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "**Sample selection as an omitted variable problem**\n",
    "\n",
    "- Regressing $y_{1}$ on $x_{1}$ using the selected sample we omit the term $\\lambda \\left( x\\delta_{2}\\right) $ \n",
    "- Heckman suggests: we estimate $\\delta_{2}$, generate $\\lambda \\left( x\\hat{\\delta}_{2}\\right)$ and include it as an regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 4: Heckman's two-step sample selection procedure\n",
    "\n",
    "**Heckit procedure, Heckman (1979):**\n",
    "1. Probit of $s_{i}$ on $x_{i} \\to \\hat{\\delta}_{2}$ and $\\lambda _{i}=\\lambda \\left( x_{i}\\hat{\\delta}_{2}\\right) $\n",
    "2. OLS of $y_{1i}$ on $x_{1i}$ and the generated regressor $\\lambda _{i}=\\lambda \\left( x_{i}\\hat{\\delta}_{2}\\right) \n",
    "\\to \\hat{\\beta}_{1},\\hat{\\gamma}_{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Test for selectivity bias following Heckman's two-step procedure\n",
    "\n",
    "When $\\gamma_{1}\\neq 0$ *asymptotic inference is complicated* for two reasons\n",
    "1. Heteroscedasticity: $Var\\left( y_{1}|x,s=1\\right) \\neq Var\\left( y_{1}|x\\right)$ is not constant.\n",
    "\n",
    "2. $\\hat{\\lambda}_{i}$ is a generated regressor.\n",
    "\n",
    "- **Non-standard inference when $\\gamma \\neq 0$**\n",
    "Covariance matrix on the form $A_{0}^{-1}B_{0}A_{0}^{-1}$ is robust for heterosedasticity, but this will not solve generated regressors problem\n",
    "- **But we can still test for selectivity bias, i.e. $H_{0}:\\gamma_{1}=0$**\n",
    "(using the usual t-statistics based on OLS standard errors)\n",
    "- **Why?:** Under $H_{0}$ we have $\\gamma_{1}=0$ and $Var\\left(y_{1}|x,s=1\\right) =Var\\left( y_{1}|x\\right) =Var\\left(\n",
    "u_{1}\\right) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to correct standard errors?\n",
    "We can correct for generated regressor problem\n",
    "- Using **asymptotic theory for **two-step M-estimators** (see 12.5.2) - brain intensive\n",
    "- **Bootstrapping the standard errors** - computer intensive, but easy to do\n",
    "\n",
    "**Alternatively use MLE approach (Partial MLE)**\n",
    "- Stronger assumptions: Need to specify joint distribution of $u_{1}$ and $v_{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Heteroscedasticity due to selection\n",
    "\n",
    "Showing that $$Var\\left(y_{1}|x,s=1\\right) \\neq Var\\left(y_{1}|x\\right) $$ would take us to far \n",
    "\n",
    "But we can get some intuition by the following example: \n",
    "\n",
    "- Suppose some values of $x$ that imply low wages. \n",
    "- These $x$'s will tend to also imply a lower probability of working \n",
    "- $\\to$ more truncation \n",
    "- $\\to$ a lower variance of the error term in a sample of workers. \n",
    "- $\\to$ variance depends on $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 5: Tobit Selection: The model\n",
    "**Structural equation**\n",
    "\n",
    "$$\n",
    "y_{1}=x_{1}\\beta _{1}+u_{1},\\  \\ E\\left( u_{1}|x_{1}\\right) =0\n",
    "$$\n",
    "\n",
    "**Selection rule**\n",
    "\\begin{eqnarray*}\n",
    "s &=&1\\left( y_{2}>0\\right) \\\\\n",
    "y_{2} &=&\\max \\left( 0,x\\delta_{2}+v_{2}\\right) \\text{, }\n",
    "\\end{eqnarray*}\n",
    "\n",
    "**Same distributional assumptions as for probit selection**\n",
    "- $x=\\left( x_{1},x_{2}\\right)$\n",
    "- $\\left( u_{1},v_{2}\\right) $ is independent of $x$\n",
    "- $v_{2}\\sim N\\left( 0,1\\right) $.\n",
    "- $u_{1},v_{2}$ are mutually dependent with $E\\left( u_{1}|v_{2}\\right)=\\gamma_{1}v_{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 5: Tobit selection- two-step estimation procedure\n",
    "\n",
    "As before\n",
    "$$\n",
    "E\\left( y_{1}|x,s=1\\right) =x_{1}\\beta _{1}+\\gamma_{1}v_{2}\n",
    "$$\n",
    "\n",
    "**This suggest the two-step estimation procedure**\n",
    "1. Estimate $\\delta_{2}$ in the censored Tobit of $y_{2i}$ on $x_{i}$ and compute the residual $\\hat{v}_{2i}$\n",
    "1. Using observations with $y_{2i}>0$ estimate $\\beta _{1}$ and $\\gamma_{1}$ by the OLS regression: \n",
    " \n",
    " $y_{1i}$ on $x_{1i}$ and $\\hat{v}_{2i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 5: A few remarks on Tobit selection\n",
    "\n",
    "**Test for selectivity bias: $H_{0}:\\gamma_{1}=0$**\n",
    "- use t-statistic based on OLS standard errors (valid under the null of no selection).\n",
    "\n",
    "- we can estimate the residuals $\\hat{v}_{2}$ directly\n",
    "\n",
    "  $\\to $we do not have to compute the inverse Mills ratio\n",
    "\n",
    "- **Exclusion restriction not required** allowing $x_{1}=x$ causes no problem , since $v_{2}$ always has separate variation due to the variation in $y_{2}$\n",
    "\n",
    "The vast majority of empirical studies use probit selection rather than Tobit selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exclusion restrictions (required for probit selection)\n",
    "\n",
    "**Exclusion restriction**: \n",
    "- $x_{2}$ is assumed to affect $s$, but not $y_{1}$.\n",
    "- crucial for identification\n",
    "\n",
    "In principle, $\\beta $ is identified in Heckman's sample selection model without an exclusion restriction\n",
    "- BUT only because of the nonlinearity of the inverse Mills ratio.\n",
    "- $\\to$ we do NOT want to heavily rely on a parametric assumption \n",
    "- $\\to$ we should NOT estimate Heckman's sample selection model without an exclusion restriction\n",
    "- often the inverse Mills ratio for a large part of its distribution is fairly linear (leading to multicollinearity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exclusion restrictions\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitation: Hard to find exclusion restrictions\n",
    "\n",
    "- I can think of any exclusion restriction, which is not subject to relevant critique\n",
    "- Huge limitation of this approach\n",
    "- Finding exclusion restrictions is an \"art\"\n",
    "\n",
    "**Not possible to validity of exclusion restriction**\n",
    "- Why not simply test whether the instrument is significant in the structural equation?\n",
    "- This is NOT a validation of the exclusion restriction since we are estimating on the selected sample\n",
    "- even if it is insignificant we cannot be sure that it has no effect on the population equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Wage equation for women\n",
    "\n",
    "- The classical exclusion restriction: Existence of **small children**\n",
    "- **Identifying assumption**: having small children has an effect on women's participation decision, but not on their wage offer\n",
    "\n",
    "\n",
    "- **Critique of exclusion restriction:** \n",
    "\t- Small children can imply that the woman is restricted to working in jobs with fewer hours and lower wage\n",
    "\t- Having children is an intertemporal decision: future births may depend on current wage offers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Wage equation for women\n",
    "\n",
    "Alternative exclusion restriction:\n",
    "- Indicator for guarantied **access to day care** (In Danish'pasningsgaranti') or the degree of subsidized day care\n",
    "- Simonsen (2008) considers female labor supply and day care provision. \n",
    "\n",
    "**Critique of exclusion restriction:**\n",
    "- Tiebout model: Most productive women may choose to live where they can be sure to send their children to day care.\n",
    "- Availability of day care is correlated with both labor supply and wages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Wage equation for women\n",
    "\n",
    "Another classic exclusion restriction: *Husband's income*\n",
    "\n",
    "**Identifying assumption:** Husband reduces wife's labor supply, but has no effect on her wages.\n",
    "\n",
    "**Critique of exclusion restriction:**\n",
    "- Poor exclusion restriction if there is positive assortative matching in the marriage market (high productive men and high productive women match)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Exclusion restrictions in dynamic wage equation\n",
    "\n",
    "**Vella and Verbeek (1998):** Wage equation with dynamic labor supply\n",
    "\n",
    "**Exclusion restriction:**\n",
    "- lagged labor market participation\n",
    "\n",
    "**Identifying assumption:** Wage equation is static, but the participation decision is dynamic\n",
    "\n",
    "**Critique of exclusion restriction:**: \n",
    "- no participation previous period due to stress. \n",
    "- $\\Rightarrow $ smaller probability of participating this year due to state dependence\n",
    "- or perhaps since the worker still cannot work due to stress\n",
    "\n",
    " If worker returns: less likely to take-up a stressful job (smaller workload $\\Rightarrow$  lower wage)\n",
    "\n",
    "\n",
    " **Hence, lagged participation may affect current wage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Concluding remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concluding remarks\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "date": 1602643870.398518,
  "filename": "38_optimization.rst",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "title": "Econometrics B #13"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
